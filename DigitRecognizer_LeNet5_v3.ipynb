{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DigitRecognizer LeNet5 Full MNIST\n",
    "#\n",
    "# CNN: LetNet5 v2\n",
    "#\n",
    "# Kaggle Scoring: 0.99900, Position #30 (Top 2%) (21/1/2018)\n",
    "#\n",
    "# Seed: 1\n",
    "# Epochs: 200\n",
    "# Batch Size: 200\n",
    "# Train metrics: {'recall': 1.0, 'loss': 1.3700266e-05, 'global_step': 114951, 'f1_score': 1.0, 'accuracy': 1.0, 'precision': 1.0}\n",
    "# Val metrics: {'recall': 0.99955761, 'loss': 0.040703569, 'global_step': 114951, 'f1_score': 0.99966816871350095, 'accuracy': 0.99360001, 'precision': 0.99977875}\n",
    "# Test metrics: {'recall': 0.99955654, 'loss': 0.032167114, 'global_step': 114951, 'f1_score': 0.99955654144287098, 'accuracy': 0.99409997, 'precision': 0.99955654}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "\n",
    "BATCH_SIZE = 200\n",
    "EPOCHS = 200\n",
    "\n",
    "def onehot_to_dense( onehot):\n",
    "    locations = np.where(onehot)[1]\n",
    "\n",
    "    return np.reshape(locations, (-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "mnist = read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# 55,000 data points of training data (mnist.train)\n",
    "# 5,000 points of validation data (mnist.validation)\n",
    "# 10,000 points of test data (mnist.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (55000, 784)\n",
      "val shape: (5000, 784)\n",
      "test shape: (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# 70k total samples\n",
    "print(\"train shape: {}\".format(mnist.train._images.shape))    #(55000, 784)   91%\n",
    "print(\"val shape: {}\".format(mnist.validation._images.shape)) #(5000, 784)     9%\n",
    "print(\"test shape: {}\".format(mnist.test._images.shape))      #(10000, 784)   14%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (55000, 28, 28, 1)\n",
      "X_val.shape: (5000, 28, 28, 1)\n",
      "X_test.shape: (10000, 28, 28, 1)\n",
      "y_train.shape: (55000, 1)\n",
      "y_val.shape: (5000, 1)\n"
     ]
    }
   ],
   "source": [
    "#label,pixel0,pixel1,pixel2,..,pixel783\n",
    "X_train = np.reshape( mnist.train._images, (-1,28,28,1))\n",
    "X_val   = np.reshape( mnist.validation._images, (-1,28,28,1))\n",
    "X_test  = np.reshape( mnist.test._images, (-1,28,28,1))\n",
    "\n",
    "y_train = onehot_to_dense( mnist.train._labels)\n",
    "y_val   = onehot_to_dense( mnist.validation._labels)\n",
    "y_test  = onehot_to_dense( mnist.test._labels)\n",
    "\n",
    "print(\"X_train.shape: {}\".format(X_train.shape))\n",
    "print(\"X_val.shape: {}\".format(X_val.shape))\n",
    "print(\"X_test.shape: {}\".format(X_test.shape))\n",
    "\n",
    "print(\"y_train.shape: {}\".format(y_train.shape))\n",
    "print(\"y_val.shape: {}\".format(y_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model: Lenet-5 v3\n",
    "\n",
    "def cnn_model_fn( features, labels, mode):\n",
    "    # Conv1: 14x14@30\n",
    "    model = tf.layers.conv2d( \n",
    "        inputs = features[\"x\"], \n",
    "        filters = 30, \n",
    "        kernel_size = [5, 5],\n",
    "        strides = 1, \n",
    "        padding = \"SAME\",\n",
    "        activation = tf.nn.relu)\n",
    "    model = tf.layers.max_pooling2d( \n",
    "        inputs = model,\n",
    "        pool_size = [ 2, 2],\n",
    "        strides = 2)\n",
    "    \n",
    "    # Conv2: 7x7@16\n",
    "    model = tf.layers.conv2d( \n",
    "        inputs = model, \n",
    "        filters = 16, \n",
    "        kernel_size = [3, 3],\n",
    "        strides = 1, \n",
    "        padding = \"SAME\",\n",
    "        activation = tf.nn.relu)\n",
    "    model = tf.layers.max_pooling2d( \n",
    "        inputs = model,\n",
    "        pool_size = [ 2, 2],\n",
    "        strides = 2)\n",
    "\n",
    "    model = tf.layers.dropout( model, rate=0.2, training=(mode == tf.estimator.ModeKeys.TRAIN))\n",
    "    \n",
    "    model = tf.layers.flatten( inputs = model) # 784 nodes\n",
    "\n",
    "    # FC\n",
    "    model = tf.layers.dense( \n",
    "        inputs = model,\n",
    "        units = 120,\n",
    "        activation = tf.nn.relu)\n",
    "\n",
    "    model = tf.layers.dropout( model, rate=0.2, training=(mode == tf.estimator.ModeKeys.TRAIN))\n",
    "        \n",
    "    model = tf.layers.dense( \n",
    "        inputs = model,\n",
    "        units = 84,\n",
    "        activation = tf.nn.relu)\n",
    "\n",
    "    # Output (softmax)\n",
    "    logits = tf.layers.dense( model, 10, name = \"output_tensor\")\n",
    "    \n",
    "    predictions = {\n",
    "        \"classes\": tf.argmax( logits, axis=1),\n",
    "        \"probabilities\": tf.nn.softmax( logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # PREDICT\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode = mode, predictions = predictions)\n",
    "    \n",
    "    \n",
    "    #Â TRAIN\n",
    "    onehot_labels = tf.one_hot( indices = tf.cast(labels, tf.int32), depth = 10)\n",
    "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits( \n",
    "        logits=logits, \n",
    "        labels=onehot_labels), name=\"cost_tensor\")\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        train_op = tf.train.AdamOptimizer().minimize( cost, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode = mode, loss = cost, train_op = train_op)\n",
    "    \n",
    "    \n",
    "    # EVAL\n",
    "    eval_metrics_ops = {\n",
    "        \"accuracy\": tf.metrics.accuracy(\n",
    "            labels = labels, \n",
    "            predictions = predictions[\"classes\"]),\n",
    "        \n",
    "        \"precision\": tf.metrics.precision(\n",
    "            labels = labels,\n",
    "            predictions = predictions[\"classes\"]),\n",
    "        \n",
    "        \"recall\": tf.metrics.recall(\n",
    "            labels = labels,\n",
    "            predictions = predictions[\"classes\"])\n",
    "    }\n",
    "    return tf.estimator.EstimatorSpec( mode = mode, loss = cost, eval_metric_ops = eval_metrics_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model( X_train, y_train):\n",
    "    model = tf.estimator.Estimator(\n",
    "        model_fn = cnn_model_fn,\n",
    "        model_dir=\"/tmp/cnn_lenet5_v1_digits_model\")\n",
    "\n",
    "    model.train(\n",
    "        input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            x = {\"x\": X_train},\n",
    "            y = y_train,\n",
    "            batch_size = BATCH_SIZE,\n",
    "            num_epochs = EPOCHS,\n",
    "            shuffle = True))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model( model, features, labels):\n",
    "\n",
    "    def f1_score( precision, recall):\n",
    "        return 2.0 / (precision**-1 + recall**-1)\n",
    "    \n",
    "    # Model performance\n",
    "    metrics = model.evaluate(\n",
    "        input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            x = {\"x\": features},\n",
    "            y = labels,\n",
    "            batch_size = BATCH_SIZE,\n",
    "            num_epochs = 1,\n",
    "            shuffle = False))\n",
    "\n",
    "    metrics[\"f1_score\"] = f1_score( metrics[\"precision\"], metrics[\"recall\"])\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kaggle_make_prediction( model, test_filename, output_filename):\n",
    "    data_test  = np.genfromtxt( test_filename,  delimiter=',', dtype=np.float32)\n",
    "    X_test  = data_test[1:,:]              #(28000, 784)\n",
    "    X_test  = X_test.reshape((-1,28,28,1)) #(42000, 28, 28, 1)\n",
    "    X_test  = X_test / 255.0\n",
    "\n",
    "    predictions = model.predict(\n",
    "        input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            x = {\"x\": X_test},\n",
    "            num_epochs = 1,\n",
    "            shuffle = False))\n",
    "\n",
    "    csv = open( output_filename, \"w\")\n",
    "    csv.write(\"ImageId,Label\\n\")\n",
    "    for i, pred in enumerate(predictions):\n",
    "        csv.write(\"{},{}\\n\".format( i+1, pred[\"classes\"]))\n",
    "    csv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Train metrics: {'recall': 1.0, 'loss': 1.3700266e-05, 'global_step': 114951, 'f1_score': 1.0, 'accuracy': 1.0, 'precision': 1.0}\n",
      ">> Val metrics: {'recall': 0.99955761, 'loss': 0.040703569, 'global_step': 114951, 'f1_score': 0.99966816871350095, 'accuracy': 0.99360001, 'precision': 0.99977875}\n",
      ">> Test metrics: {'recall': 0.99955654, 'loss': 0.032167114, 'global_step': 114951, 'f1_score': 0.99955654144287098, 'accuracy': 0.99409997, 'precision': 0.99955654}\n"
     ]
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.set_random_seed( SEED)\n",
    "    model = train_model( X_train, y_train)\n",
    "    \n",
    "    print(\">> Train metrics: %r\"% evaluate_model( model, X_train, y_train))\n",
    "    print(\">> Val metrics: %r\"% evaluate_model( model, X_val, y_val))\n",
    "    \n",
    "    print(\">> Test metrics: %r\"% evaluate_model( model, X_test, y_test))\n",
    "    \n",
    "    kaggle_make_prediction( model, \"test.csv\", \"digit_prediction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
